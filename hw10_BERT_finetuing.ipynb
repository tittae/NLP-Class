{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"HW10_BERT_finetuing.ipynb","provenance":[],"collapsed_sections":["rLN_6EslN8F0","NS7Sjv3ae3er"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"1b1abf14d2f74e35bc932554f5dac8e2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_1bc3e4b0b779450995566d6f5ef02bc0","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_3083528320fb4d15ba8dff9dfff8889b","IPY_MODEL_50368115bd2c419c9e8b42e62d97f03a"]}},"1bc3e4b0b779450995566d6f5ef02bc0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3083528320fb4d15ba8dff9dfff8889b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_537e16e5b5064d88aec788ebe7415876","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":231508,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":231508,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_213aa3257c3047f5911e59ceb2261dc9"}},"50368115bd2c419c9e8b42e62d97f03a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_f605a9fb0b544002b54215d4afa25e12","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 232k/232k [00:00&lt;00:00, 337kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2dc942c5689043968bccd489898e76f7"}},"537e16e5b5064d88aec788ebe7415876":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"213aa3257c3047f5911e59ceb2261dc9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f605a9fb0b544002b54215d4afa25e12":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"2dc942c5689043968bccd489898e76f7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4c6cabf962ea46089d79908f880e4f42":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_8a1749f47c7849c9a823ca76d7052cd4","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_4fb25f1e845645ae8cf415958253481a","IPY_MODEL_a9bd2ef6943b40f894c26b01d8b69d2b"]}},"8a1749f47c7849c9a823ca76d7052cd4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4fb25f1e845645ae8cf415958253481a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_06318ec51f41490d83e2bbed3421a0a7","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":466062,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":466062,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ba9b24e31f824a9788cd9a4a28941710"}},"a9bd2ef6943b40f894c26b01d8b69d2b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e8934537da674ceeaad6069ad2c8abd0","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 466k/466k [00:14&lt;00:00, 33.0kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c4875b57f39c47769663166ea3e98345"}},"06318ec51f41490d83e2bbed3421a0a7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"ba9b24e31f824a9788cd9a4a28941710":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e8934537da674ceeaad6069ad2c8abd0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"c4875b57f39c47769663166ea3e98345":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1d370ad6096f4b00852bd2063eaec353":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_db48b4d928e44d8a926c66cecbfffb64","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_b3704501f3c94837bb9e3f30c9d0679d","IPY_MODEL_78b126c0edbc469b9de5039c9847823c"]}},"db48b4d928e44d8a926c66cecbfffb64":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b3704501f3c94837bb9e3f30c9d0679d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_9808c9c3be234911b408bfee87f2703e","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":28,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":28,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e6adb6f615ee40cb8759b4744507da29"}},"78b126c0edbc469b9de5039c9847823c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_7b96588d993c4c9b939e4c40aa808975","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 28.0/28.0 [00:00&lt;00:00, 235B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_93414015ba45475881396754d72efe05"}},"9808c9c3be234911b408bfee87f2703e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"e6adb6f615ee40cb8759b4744507da29":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7b96588d993c4c9b939e4c40aa808975":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"93414015ba45475881396754d72efe05":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e3234ede648e4e10bbec226fe17680e8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_0d8416d992294eb68b87d060d50c678f","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_caa130d049034f3080e600dccace3170","IPY_MODEL_6cedaffad8294a3185534609f22da379"]}},"0d8416d992294eb68b87d060d50c678f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"caa130d049034f3080e600dccace3170":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_0cfd0395fe614bf8ad47d0f0b18ce7a5","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":1000,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1000,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b0d9bdcc7a7d4d1cbbae77400c7aa842"}},"6cedaffad8294a3185534609f22da379":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_719b9dff3ec84a759944e930f34d75d6","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1000/1000 [23:36&lt;00:00,  1.42s/it]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f11081c6bbb9426797efe8f94c1844c3"}},"0cfd0395fe614bf8ad47d0f0b18ce7a5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"b0d9bdcc7a7d4d1cbbae77400c7aa842":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"719b9dff3ec84a759944e930f34d75d6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"f11081c6bbb9426797efe8f94c1844c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8f4acbfa1da74be3a5b0cfc4949867d8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_9b94b6b3dd8a4adaa12a78a38e2a997a","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_7fbee391ccd34b1d8cb7b51201c561d6","IPY_MODEL_b1882c5d039a414cb44664caca2ad800"]}},"9b94b6b3dd8a4adaa12a78a38e2a997a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7fbee391ccd34b1d8cb7b51201c561d6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_45dceb81dcfb4966a1fd1932dbae2036","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":1000,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1000,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d05e2a5069384c86b259d2d46e7483e8"}},"b1882c5d039a414cb44664caca2ad800":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_3c694ff844a345ea9e419308ef8e6f36","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1000/1000 [09:18&lt;00:00,  1.79it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4053f5f52ff441c6ace44ba3dbd779b1"}},"45dceb81dcfb4966a1fd1932dbae2036":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"d05e2a5069384c86b259d2d46e7483e8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3c694ff844a345ea9e419308ef8e6f36":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"4053f5f52ff441c6ace44ba3dbd779b1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"DazlADInMRhZ"},"source":["##  HW10: BERT fintuning. \n","\n","In this exercise, you are going to learn how to perform fine-tuning on a transformer-based model. First, we will provide a tutorial on fine-tuning the Large Movie Review Dataset (IMDB dataset) using distilBERT (https://arxiv.org/abs/1910.01108). After that, you have to complete the exercise by fine-tuning on the TRUE call-center dataset (HW6). This homework is based on the Hugging Face tutorial (https://huggingface.co/transformers/custom_datasets.html)."]},{"cell_type":"markdown","metadata":{"id":"tRu2NjxvOSzu"},"source":["### 1. Install transformers library form Hugging Face"]},{"cell_type":"code","metadata":{"id":"AItukECo4Yyz","executionInfo":{"status":"ok","timestamp":1618842940251,"user_tz":-420,"elapsed":17108,"user":{"displayName":"Wisaroot Lertthaweedech","photoUrl":"","userId":"18087249014958546013"}}},"source":["# !pip install torch==1.4.0\n","_ = !pip install transformers\n","_ = !pip install pythainlp\n","_ = !pip install sentencepiece"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XeeinFe5O52s"},"source":["### 2. Download Large Movie Review Dataset "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MZx3kKjoBVcH","executionInfo":{"status":"ok","timestamp":1618842947399,"user_tz":-420,"elapsed":24196,"user":{"displayName":"Wisaroot Lertthaweedech","photoUrl":"","userId":"18087249014958546013"}},"outputId":"b2f5f520-e4ea-4951-b456-483bcd295c7d"},"source":["!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n","!tar -xf aclImdb_v1.tar.gz"],"execution_count":2,"outputs":[{"output_type":"stream","text":["--2021-04-19 14:35:39--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n","Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n","Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 84125825 (80M) [application/x-gzip]\n","Saving to: ‘aclImdb_v1.tar.gz’\n","\n","aclImdb_v1.tar.gz   100%[===================>]  80.23M  56.6MB/s    in 1.4s    \n","\n","2021-04-19 14:35:40 (56.6 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rLN_6EslN8F0"},"source":["### 3. Preprocess the dataset  \n","Large Movie Review Dataset  is a dataset for binary sentiment classification. The input of this dataset is a movie review with its sentiment as a ground truth"]},{"cell_type":"code","metadata":{"id":"a1rgQw7nBXcH","executionInfo":{"status":"ok","timestamp":1618842949635,"user_tz":-420,"elapsed":26430,"user":{"displayName":"Wisaroot Lertthaweedech","photoUrl":"","userId":"18087249014958546013"}}},"source":["from pathlib import Path\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","\n","def read_imdb_split(split_dir):\n","    split_dir = Path(split_dir)\n","    texts = []\n","    labels = []\n","    for label_dir in [\"pos\", \"neg\"]:\n","        for text_file in (split_dir/label_dir).iterdir():\n","            texts.append(text_file.read_text())\n","            labels.append(0 if label_dir is \"neg\" else 1)\n","\n","    return texts, labels\n","\n","train_texts, train_labels = read_imdb_split('aclImdb/train')\n","test_texts, test_labels = read_imdb_split('aclImdb/test')\n","train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2)"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h3RUR7lrBb8g","executionInfo":{"status":"ok","timestamp":1618842949636,"user_tz":-420,"elapsed":26427,"user":{"displayName":"Wisaroot Lertthaweedech","photoUrl":"","userId":"18087249014958546013"}},"outputId":"c2d4bf5c-b8d1-41af-dee6-706ebe28d280"},"source":["print(\"Unique label is {}, nb. of train data = {}, test_data = {}\".format(np.unique(train_labels), len(train_texts), len(test_texts)))\n","for i in range(5):\n","  print(\"Data = {}\".format(train_texts[i]))\n","  print(\"Label = {}\".format(train_labels[i]))"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Unique label is [0 1], nb. of train data = 20000, test_data = 25000\n","Data = This is a film that takes some digesting. On the one hand, we are offered a tough outward shell, a story that does not only derive the Catholic Church, but does so foolishly, and uninformed. On an inner layer, we are offered a story of orthodoxy over orthopraxis, and what happens when people follow blindly a faith that they must not understand.<br /><br />At first glance, it appeared this was supposed to be a comedy. If so, then Mr. Durang needs to open a dictionary, because he clearly does not know the meaning of the word. The jokes are pale; the humor is awkward and poorly delivered. In particular, Ms. Keaton's performance is flighty and over the top, well below the quality of her Annie Hall and Sleeper days. Jennifer Tilly is again the model of stridence, with her hi-pitched voice and whining style. All of this could be forgiven if it weren't for the last 20 minutes of this movie, that evidently was a controversial play made in 1981.<br /><br />***Careful, spoilers ahead***<br /><br />It all starts with the appearance of four former students of Sister Mary Ignatius (Ignatius, by the way, is a male name, and a nun would not adopt it after her vows under any circumstance simply due to that fact, just to show you how much tireless research went into the project to begin with.) When they all admit that they don't live up to the church's teachings, the sister proceeds to become irrational and abuse them in a manner the audience is to believe she did way back when in the corny, all-too-cliché sepia-tone flashbacks. When one of them admits to having two abortions, the nun becomes even more abusive, until the pupil pulls out a gun. After wrestling it away from her, the nun kills the pupil, presumably in self-defense. She then goes on a screaming rampage, killing a gay former student because of his sins. The last shot is of the dead female pupil lying in a Christ-like pose as a shadow of a cross hangs over her. Can you say `heavy handed?' I knew you could!<br /><br />I know there have been abusive nuns in the past, and I know many people have been emotionally harmed as a result, but this imagery is fed down our throats in almost every other shot in this train wreck of a movie. I have heard from the writer and the director that this is a film about hysteria and why one should not follow the orthodoxy so religiously, no pun intended. This explanation is hard to swallow, though, simply because we are never given an authoritative viewpoint that is not biased against the catholic faith in one way or another. This film is simply anti-Catholic tripe, which in the name of fairness and equality, is mean spirited and hateful.<br /><br />This is a film I would recommend for a catholic, namely to awaken him or her to the realities of what cynicism and ignorance they face today. If it were `Rabbi Ray explains it all' or `Imam Muhammad explains it all', there would be rioting in the streets and Showtime would lose all of its subscription. But, sadly, because this is a film that strikes out against what is perceived to be the majority, it is accepted and even applauded by those who share the same spiteful point of view.<br /><br />I certainly hope every member of that cast was a practicing catholic, so it wasn't just ignorance that brought them to make this film.<br /><br />I give it 1.5 stars out of 5, not because of its offensive nature, but because it was poorly written, poorly directed and just a bad movie in general. Don't even waste your time.\n","Label = 0\n","Data = A really sweet movie that has some similarities to the 2001-hit \"My Sassy Girl\" but is able to enchant most of the time. The biggest applause should go to the two leads. Ha-Neul Kim is both sweet and quirky, Sang-woo Kwon is both attractive and rebellious. The chemistry between the two is very good.<br /><br />Director Kyeong-hyeong Kim uses some CG-inserts to pepper up the visuals and also offers impressive fight scenes in which Sang-woo Kwon can shine. I liked him a lot better here than in the highly overrated \"Volcano High\". And that boy has a future - those looks, those fight techniques, and a romantic lead. Not bad.<br /><br />Well, I can make it short: Nice film. My rating: 7/10\n","Label = 1\n","Data = \"Plan B\" is strictly by-the-numbers fare except for one thing. I surprisingly found it to be rather insulting.<br /><br />Jon Cryer is the \"star\" of this film and plays his usual, smarmy, egotistical, snotty character that was actually endearing in \"Pretty In Pink\" and has NOT been amusing ever since. Grating doesn't even begin to describe his performance. Ricky (Mark Matheisen) is a muscular, blonde, struggling actor who (gasp!) is only worried about his hair and getting laid. Talk about a stock character...ugh. At least the other three characters are somewhat engaging. Lisa Darr and Lance Guest play a grounded, optimistic, caring couple who are struggling to conceive. Since they are not whiny drama queens, however, their roles are apparently considered boring and they aren't given enough screen time. Sara Mornell rounds out the cast by playing Gina, your usual nice and good-looking young woman who just can't seem to find the right partner in love. I've seen this character a million times before but at least her performance overcomes some of the shortfalls caused by the predictability of her situation.<br /><br />What startled me about this film was its juvenile promotion of stereotypes. They introduced a Russian character for the sole purpose of mocking him and making fun of the way he talked. He was portrayed as being wild, ignorant and amazingly shallow. They were just getting warmed up though for the usual nonsense about gays. Gina decides to be gay for a while since she isn't having any luck with guys. Honestly. That wasn't that bad except they really went overboard when Gina brought a lesbian to a Christmas party her friends were throwing. Her lesbian date had very short hair (like I'm sure all lesbians do). She also got quite upset (like I'm sure all lesbians do) when Gina had the nerve to put on lipstick(!). Finally, her date goes around the party hitting on just about every woman there and mouths off when Gina expresses her disappointment. Of course, we all know how gay people can't stay faithful for so much as a couple of hours much less months or even years, right? (Please note the sarcasm in that statement. Thank you.)<br /><br />This film was based on a tired and predictable premise to begin with but Cryer's unlikable performance combined with the idiotic stereotyping sinks this movie to the lower depths of cinema. 2/10\n","Label = 0\n","Data = I rented it because the second segment traumatized me as a little kid. I snuck downstairs really early one morning, started watching HBO, and The Raft (segment 2) terrorized me good. This time around, I still enjoyed The Raft, although I couldn't tell whether it was for nostalgic reasons or if it was actually a good short. The other two segments were complete trash. I can't believe a producer somewhere payed to make this junk. All I've accomplished by watching this was to ruin one more childhood memory. Creepshow 2 will now join Rad among my list of tainted childhood classics. 4/10\n","Label = 0\n","Data = The sexploitation movie era of the late sixties and early seventies began with the allowance of gratuitous nudity in mainstream films and ended with the legalization of hardcore porn. It's peak years were between 1968 and 1972. One of the most loved and talented actresses of the era was Monica Gayle, who had a small but fanatic cult of followers. She was actually able to act, unlike many who filled the lead roles of these flicks, and her subsequent credits proved it. And her seemingly deliberate fade into obscurity right when her career was taking off only heightens her mystique.<br /><br />Gary Graver, the director, was also a talent; probably too talented for the sexploitation genre, and his skill, combined with Monica Gayle's screen presence, makes Sandra, the Making of a Woman, a pleasantly enjoyable experience. The film never drags and you won't have your finger pressed on the fast-forward button.\n","Label = 1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zKff_Vz4QDcs"},"source":["After the dataset is processed, we tokenize each input sentence. This tokenizer has a start token of '[CLS'] (id 101) and a seperator token '[SEP]' (id 102) at the end of each sentence. If the word is an Out-of-vocabulary word (OOV), the token id is 100. The tokenized output has the following format :\n","\n","```python\n","{\n","  'input_ids': List[List[Int]]. List of tokenized input sentence.\n","  'attention_mask' : List[List[Int]].  List of masked token. See cell [7] for example.\n","}\n","```"]},{"cell_type":"code","metadata":{"id":"lNhdSFz0CjxY","colab":{"base_uri":"https://localhost:8080/","height":164,"referenced_widgets":["1b1abf14d2f74e35bc932554f5dac8e2","1bc3e4b0b779450995566d6f5ef02bc0","3083528320fb4d15ba8dff9dfff8889b","50368115bd2c419c9e8b42e62d97f03a","537e16e5b5064d88aec788ebe7415876","213aa3257c3047f5911e59ceb2261dc9","f605a9fb0b544002b54215d4afa25e12","2dc942c5689043968bccd489898e76f7","4c6cabf962ea46089d79908f880e4f42","8a1749f47c7849c9a823ca76d7052cd4","4fb25f1e845645ae8cf415958253481a","a9bd2ef6943b40f894c26b01d8b69d2b","06318ec51f41490d83e2bbed3421a0a7","ba9b24e31f824a9788cd9a4a28941710","e8934537da674ceeaad6069ad2c8abd0","c4875b57f39c47769663166ea3e98345","1d370ad6096f4b00852bd2063eaec353","db48b4d928e44d8a926c66cecbfffb64","b3704501f3c94837bb9e3f30c9d0679d","78b126c0edbc469b9de5039c9847823c","9808c9c3be234911b408bfee87f2703e","e6adb6f615ee40cb8759b4744507da29","7b96588d993c4c9b939e4c40aa808975","93414015ba45475881396754d72efe05"]},"executionInfo":{"status":"ok","timestamp":1618842952946,"user_tz":-420,"elapsed":29733,"user":{"displayName":"Wisaroot Lertthaweedech","photoUrl":"","userId":"18087249014958546013"}},"outputId":"15755621-fc6d-4e4d-9ff3-92312cf08e92"},"source":["from transformers import DistilBertTokenizerFast\n","tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"],"execution_count":5,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1b1abf14d2f74e35bc932554f5dac8e2","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4c6cabf962ea46089d79908f880e4f42","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466062.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1d370ad6096f4b00852bd2063eaec353","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=28.0, style=ProgressStyle(description_w…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Eg0h4BDsDx9J","executionInfo":{"status":"ok","timestamp":1618842952946,"user_tz":-420,"elapsed":29729,"user":{"displayName":"Wisaroot Lertthaweedech","photoUrl":"","userId":"18087249014958546013"}},"outputId":"7e2b39ce-74ca-4674-d062-48b473100373"},"source":["tokenizer([ '[CLS] a' ], truncation=True, padding=True)"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': [[101, 101, 1037, 102]], 'attention_mask': [[1, 1, 1, 1]]}"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M5rrUE91E_Ke","executionInfo":{"status":"ok","timestamp":1618842952947,"user_tz":-420,"elapsed":29726,"user":{"displayName":"Wisaroot Lertthaweedech","photoUrl":"","userId":"18087249014958546013"}},"outputId":"f410f14c-896e-492f-eb58-3d08bc3d199e"},"source":["tokenizer( ['Pine apple apple pen  หมา ไก่', 'a b'], truncation=True, padding=True)"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': [[101, 7222, 6207, 6207, 7279, 100, 100, 102], [101, 1037, 1038, 102, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 0, 0, 0, 0]]}"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kGGxJLmdFcFB","executionInfo":{"status":"ok","timestamp":1618842952948,"user_tz":-420,"elapsed":29722,"user":{"displayName":"Wisaroot Lertthaweedech","photoUrl":"","userId":"18087249014958546013"}},"outputId":"8e8a5d0d-fa43-4a74-f647-63380138a800"},"source":["a = tokenizer(train_texts[:2], truncation=True, padding=True)\n","print(a)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["{'input_ids': [[101, 2023, 2003, 1037, 2143, 2008, 3138, 2070, 17886, 2075, 1012, 2006, 1996, 2028, 2192, 1010, 2057, 2024, 3253, 1037, 7823, 15436, 5806, 1010, 1037, 2466, 2008, 2515, 2025, 2069, 18547, 1996, 3234, 2277, 1010, 2021, 2515, 2061, 13219, 2135, 1010, 1998, 4895, 2378, 29021, 1012, 2006, 2019, 5110, 6741, 1010, 2057, 2024, 3253, 1037, 2466, 1997, 26582, 2058, 2030, 2705, 7361, 2527, 9048, 2015, 1010, 1998, 2054, 6433, 2043, 2111, 3582, 25734, 1037, 4752, 2008, 2027, 2442, 2025, 3305, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2012, 2034, 6054, 1010, 2009, 2596, 2023, 2001, 4011, 2000, 2022, 1037, 4038, 1012, 2065, 2061, 1010, 2059, 2720, 1012, 22959, 2290, 3791, 2000, 2330, 1037, 9206, 1010, 2138, 2002, 4415, 2515, 2025, 2113, 1996, 3574, 1997, 1996, 2773, 1012, 1996, 13198, 2024, 5122, 1025, 1996, 8562, 2003, 9596, 1998, 9996, 5359, 1012, 1999, 3327, 1010, 5796, 1012, 17710, 22436, 1005, 1055, 2836, 2003, 3462, 2100, 1998, 2058, 1996, 2327, 1010, 2092, 2917, 1996, 3737, 1997, 2014, 8194, 2534, 1998, 24372, 2420, 1012, 7673, 6229, 2100, 2003, 2153, 1996, 2944, 1997, 18045, 5897, 1010, 2007, 2014, 7632, 1011, 8219, 2376, 1998, 1059, 20535, 3070, 2806, 1012, 2035, 1997, 2023, 2071, 2022, 24280, 2065, 2009, 4694, 1005, 1056, 2005, 1996, 2197, 2322, 2781, 1997, 2023, 3185, 1010, 2008, 15329, 2001, 1037, 6801, 2377, 2081, 1999, 3261, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1008, 1008, 1008, 6176, 1010, 27594, 2545, 3805, 1008, 1008, 1008, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2009, 2035, 4627, 2007, 1996, 3311, 1997, 2176, 2280, 2493, 1997, 2905, 2984, 26841, 1006, 26841, 1010, 2011, 1996, 2126, 1010, 2003, 1037, 3287, 2171, 1010, 1998, 1037, 16634, 2052, 2025, 11092, 2009, 2044, 2014, 16495, 2104, 2151, 25652, 3432, 2349, 2000, 2008, 2755, 1010, 2074, 2000, 2265, 2017, 2129, 2172, 12824, 3238, 2470, 2253, 2046, 1996, 2622, 2000, 4088, 2007, 1012, 1007, 2043, 2027, 2035, 6449, 2008, 2027, 2123, 1005, 1056, 2444, 2039, 2000, 1996, 2277, 1005, 1055, 12209, 1010, 1996, 2905, 10951, 2000, 2468, 23179, 1998, 6905, 2068, 1999, 1037, 5450, 1996, 4378, 2003, 2000, 2903, 2016, 2106, 2126, 2067, 2043, 1999, 1996, 9781, 2100, 1010, 2035, 1011, 2205, 1011, 18856, 17322, 19802, 2401, 1011, 4309, 28945, 1012, 2043, 2028, 1997, 2068, 14456, 2000, 2383, 2048, 11324, 2015, 1010, 1996, 16634, 4150, 2130, 2062, 20676, 1010, 2127, 1996, 11136, 8005, 2041, 1037, 3282, 1012, 2044, 4843, 2009, 2185, 2013, 2014, 1010, 1996, 16634, 8563, 1996, 11136, 1010, 10712, 1999, 2969, 1011, 3639, 1012, 2016, 2059, 3632, 2006, 1037, 7491, 29216, 1010, 4288, 1037, 5637, 2280, 3076, 2138, 1997, 2010, 15516, 1012, 1996, 2197, 2915, 2003, 1997, 1996, 2757, 2931, 11136, 4688, 1999, 1037, 4828, 1011, 2066, 13382, 2004, 1037, 5192, 1997, 1037, 2892, 17991, 2058, 2014, 1012, 2064, 2017, 2360, 1036, 3082, 4375, 1029, 1005, 1045, 2354, 2017, 2071, 999, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1045, 2113, 2045, 2031, 2042, 20676, 16752, 1999, 1996, 2627, 1010, 1998, 1045, 2113, 2116, 2111, 2031, 2042, 14868, 25596, 2004, 1037, 2765, 1010, 2021, 2023, 13425, 2003, 7349, 102], [101, 1037, 2428, 4086, 3185, 2008, 2038, 2070, 12319, 2000, 1996, 2541, 1011, 2718, 1000, 2026, 21871, 6508, 2611, 1000, 2021, 2003, 2583, 2000, 4372, 14856, 2102, 2087, 1997, 1996, 2051, 1012, 1996, 5221, 20737, 2323, 2175, 2000, 1996, 2048, 5260, 1012, 5292, 1011, 11265, 5313, 5035, 2003, 2119, 4086, 1998, 21864, 15952, 1010, 6369, 1011, 15854, 6448, 2239, 2003, 2119, 8702, 1998, 22614, 1012, 1996, 6370, 2090, 1996, 2048, 2003, 2200, 2204, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2472, 18712, 10242, 2290, 1011, 1044, 6672, 5063, 5035, 3594, 2070, 1039, 2290, 1011, 19274, 2015, 2000, 11565, 2039, 1996, 26749, 1998, 2036, 4107, 8052, 2954, 5019, 1999, 2029, 6369, 1011, 15854, 6448, 2239, 2064, 12342, 1012, 1045, 4669, 2032, 1037, 2843, 2488, 2182, 2084, 1999, 1996, 3811, 2058, 9250, 1000, 12779, 2152, 1000, 1012, 1998, 2008, 2879, 2038, 1037, 2925, 1011, 2216, 3504, 1010, 2216, 2954, 5461, 1010, 1998, 1037, 6298, 2599, 1012, 2025, 2919, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2092, 1010, 1045, 2064, 2191, 2009, 2460, 1024, 3835, 2143, 1012, 2026, 5790, 1024, 1021, 1013, 2184, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Sr_2xxBDCu0K","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618842987946,"user_tz":-420,"elapsed":64717,"user":{"displayName":"Wisaroot Lertthaweedech","photoUrl":"","userId":"18087249014958546013"}},"outputId":"6b53d44c-ca7c-40a5-a469-7f08f0827fce"},"source":["train_encodings = tokenizer(train_texts, add_special_tokens=True,\n","            max_length=512,\n","            pad_to_max_length=True,\n","            return_token_type_ids=True,truncation=True\n","        )\n","val_encodings = tokenizer(val_texts, add_special_tokens=True,\n","            max_length=512,\n","            pad_to_max_length=True,\n","            return_token_type_ids=True,truncation=True\n","        )\n","test_encodings = tokenizer(test_texts, add_special_tokens=True,\n","            max_length=512,\n","            pad_to_max_length=True,\n","            return_token_type_ids=True,truncation=True\n","        )"],"execution_count":9,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"H8ZCqfOnYzUl"},"source":["Convert the dataset into training format. You can see the training input format of distilBERT is in https://huggingface.co/transformers/model_doc/distilbert.html. "]},{"cell_type":"code","metadata":{"id":"6-0jjUGXYsil","executionInfo":{"status":"ok","timestamp":1618842992171,"user_tz":-420,"elapsed":68940,"user":{"displayName":"Wisaroot Lertthaweedech","photoUrl":"","userId":"18087249014958546013"}}},"source":["train_data = [np.array(train_encodings['input_ids']), np.array(train_encodings['attention_mask'])]\n","val_data = [np.array(val_encodings['input_ids']), np.array(val_encodings['attention_mask'])]\n","test_data = [np.array(test_encodings['input_ids']), np.array(test_encodings['attention_mask'])]"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o3yXNHrOTQ0T"},"source":["### 4. Model fine-tuning\n","The model we used for fine-tuning is distilBERT (https://arxiv.org/abs/1910.01108), which is a smaller model distilled from the original BERT. Knowledge distillation is a well-known trick for improving the performance of a small model by learning an estimated uncertainty from a larger model instead of using a hard-label. If you want to know more about knowledge distillation, read https://arxiv.org/abs/1503.02531."]},{"cell_type":"markdown","metadata":{"id":"Ci8UytuJQ_34"},"source":["#### Model Initialization"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":511},"id":"jTPTIED3-0wf","executionInfo":{"status":"error","timestamp":1618843595095,"user_tz":-420,"elapsed":1088,"user":{"displayName":"Wisaroot Lertthaweedech","photoUrl":"","userId":"18087249014958546013"}},"outputId":"1b34da2f-8df9-48b7-8386-cc6d5cf48f3d"},"source":["from transformers import DistilBertForSequenceClassification\n","import torch\n","\n","model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels= 2)\n","model = torch.nn.DataParallel(model.cuda(), device_ids=[0])\n","\n","LEARNING_RATE =  1e-5\n","loss_fn = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n"],"execution_count":23,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-8162bea5c730>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDistilBertForSequenceClassification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDistilBertForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'distilbert-base-uncased'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/distilbert/__init__.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers.models.distilbert.modeling_distilbert'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"cell_type":"markdown","metadata":{"id":"rXUKnyIaRDAM"},"source":["#### Set up training generator\n","\n","In contrast to model.fit which you have used in the previous lab. A more common way to feed the data is to use a generator. It is more memory-efficient than model.fit as the data is only quired when the iterator executes. For example, you can set the generator to load the image from the folder when called instead of storing all of them in the RAM. An example below is a way to create a simple generator, which aggregate the data points into a batch. Both PyTorch and TensorFlow also has a utility module for creating a generator (torch.utils.data.DataLoader for Torch and tf.data.Dataset for Tensorflow) "]},{"cell_type":"code","metadata":{"id":"-fGIxUdCQ_Mm"},"source":["def batch_data_generator(data, label, bs = 8, training = True):\n","  while(True):\n","    X1= []\n","    X2 = []\n","    Y = []\n","    from sklearn.utils import shuffle\n","    ids, masks = data[0], data[1]\n","    if(training):\n","      ids, masks, label = shuffle(ids, masks, label, random_state = 42)\n","    for a, b, c in zip(ids, masks, label):\n","      X1.append(a)\n","      X2.append(b)\n","      Y.append(c)\n","      if(len(X1) == bs):\n","        yield [np.array(X1), np.array(X2)], np.array(Y)\n","        X1= []\n","        X2 = []\n","        Y = []\n","    if(len(X1) > 0):\n","      yield [np.array(X1), np.array(X2)], np.array(Y)\n","    if(not training):\n","      yield None\n","      break\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yMADXyexR5Vc"},"source":["train_generator = batch_data_generator(train_data, np.array(train_labels, dtype = np.int), training = True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"13zA5yZQi_TI","executionInfo":{"status":"ok","timestamp":1617454800725,"user_tz":-420,"elapsed":108790,"user":{"displayName":"Wisaroot Lertthaweedech","photoUrl":"","userId":"18087249014958546013"}},"outputId":"2c648b8c-4e2d-4482-bce2-2cf943e9f5f9"},"source":["dummy_generator = batch_data_generator(train_data, np.array(train_labels, dtype = np.int), training = True)\n","X_dummy, Y_dummy = next(dummy_generator)\n","print(X_dummy[0].shape, X_dummy[1].shape, Y_dummy.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(8, 512) (8, 512) (8,)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qVs7aFHDbeQf"},"source":["#### Start Fine-tuning"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":491,"referenced_widgets":["e3234ede648e4e10bbec226fe17680e8","0d8416d992294eb68b87d060d50c678f","caa130d049034f3080e600dccace3170","6cedaffad8294a3185534609f22da379","0cfd0395fe614bf8ad47d0f0b18ce7a5","b0d9bdcc7a7d4d1cbbae77400c7aa842","719b9dff3ec84a759944e930f34d75d6","f11081c6bbb9426797efe8f94c1844c3"]},"id":"xirCCZ6wR-UZ","executionInfo":{"status":"ok","timestamp":1617455827540,"user_tz":-420,"elapsed":1135597,"user":{"displayName":"Wisaroot Lertthaweedech","photoUrl":"","userId":"18087249014958546013"}},"outputId":"2076ebe6-b7d3-428f-a62b-2d895b626039"},"source":["device = \"cuda:0\"\n","from tqdm import tqdm_notebook\n","from sklearn.metrics import accuracy_score\n","from collections import deque \n","\n","train_acc_stat =  deque(maxlen = 100)\n","train_loss_stat =  deque(maxlen = 100)\n","\n","for step in  tqdm_notebook(range(1000)):\n","    X, Y = next(train_generator)\n","    ids = torch.tensor(X[0], dtype = torch.long, device = device)\n","    mask = torch.tensor(X[1], dtype = torch.long, device = device)\n","    targets = torch.tensor(Y, dtype = torch.long).to(device)\n","\n","    optimizer.zero_grad()\n","    outputs = model(ids, mask)\n","    loss = loss_fn(outputs['logits'], targets)\n","    \n","    loss.backward()\n","    optimizer.step()\n","\n","    with torch.no_grad():\n","      train_acc = accuracy_score(Y, outputs['logits'].argmax(axis = 1).cpu().detach().numpy() )\n","      train_loss = loss.cpu().detach().numpy()\n","      train_acc_stat.append(train_acc)\n","      train_loss_stat.append(train_loss)\n","\n","    if (step + 1) %100==0:\n","      print(\"iter = {} train_acc = {}\".format(step, np.array(train_acc_stat).mean()))\n","      print(\"iter = {} train_loss = {}\".format(step, np.array(train_loss_stat).mean()))\n","\n","\n","    if (step + 1) %500==0:\n","      #validation step\n","      with torch.no_grad():\n","        val_generator = batch_data_generator(val_data, np.array(val_labels, dtype = np.int), training = False)\n","        y_true = []\n","        y_pred = []\n","        while(True):\n","          d = next(val_generator)\n","          if(d is None): break\n","          X, Y = d\n","          ids = torch.tensor(X[0], dtype = torch.long, device = device)\n","          mask = torch.tensor(X[1], dtype = torch.long, device = device)\n","          outputs_cls = model(ids, mask)['logits'].argmax(axis = 1).cpu().detach().numpy()\n","          y_true.append(Y)\n","          y_pred.append(outputs_cls)\n","        y_true = np.concatenate(y_true)\n","        y_pred = np.concatenate(y_pred)\n","        print(\"val acc\", accuracy_score(y_true, y_pred))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n","Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n","  if __name__ == '__main__':\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e3234ede648e4e10bbec226fe17680e8","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["iter = 99 train_acc = 0.78625\n","iter = 99 train_loss = 0.5005409121513367\n","iter = 199 train_acc = 0.84875\n","iter = 199 train_loss = 0.3588992655277252\n","iter = 299 train_acc = 0.88875\n","iter = 299 train_loss = 0.3059362471103668\n","iter = 399 train_acc = 0.89125\n","iter = 399 train_loss = 0.28195422887802124\n","iter = 499 train_acc = 0.88375\n","iter = 499 train_loss = 0.2709669768810272\n","val acc 0.8662\n","iter = 599 train_acc = 0.8875\n","iter = 599 train_loss = 0.26880982518196106\n","iter = 699 train_acc = 0.91625\n","iter = 699 train_loss = 0.22099053859710693\n","iter = 799 train_acc = 0.8925\n","iter = 799 train_loss = 0.273038387298584\n","iter = 899 train_acc = 0.90875\n","iter = 899 train_loss = 0.2313779890537262\n","iter = 999 train_acc = 0.92125\n","iter = 999 train_loss = 0.2166145294904709\n","val acc 0.9026\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-rPUYXAATqVR"},"source":["## TODO \n","Compare the classification performance between the non-transformer model and the model fine-tuned using pretrained WangchanBERTa on TRUE call-center dataset (HW6). WangchanBERTa (https://arxiv.org/abs/2101.09635) is RoBERTa (https://arxiv.org/abs/1907.11692) trained on thai texts. RoBERTa is also supported in Hugging Face (https://huggingface.co/transformers/model_doc/roberta.html).\n","\n","For this homework, you may focus only on the object tag.\n","To successfully fine-tune WangchanBERTa on the TRUE call-center dataset, you should:\n","\n","1. Preprocess the dataset into the same format as the tutorial.\n","2. Tokenize the input from 1. See (https://colab.research.google.com/drive/1Kbk6sBspZLwcnOE61adAQo30xxqOQ9ko?usp=sharing&fbclid=IwAR23b8ZEoP6YxlUx7wWEu7dRCrVcyTFrZb3YSgI-nsxe_t4gy-bh8Rv5R9E#scrollTo=kAcpAdkddVQ8) for more details.\n","3. Process the tokenized input from 1. to the format that could be fed to the model.\n","4. Initialize WangchanBERTa (<b> you should choose the pretrained weight w.r.t. the tokenizer in 2.</b>)\n","5. Fine-tune the pretrained model.\n","6.  (Optional) Before fine-tuning is performed (before step 5), domain adaptation is often performed first by training a masked language model (maskLM). You can train maskLM by following this guideline (https://huggingface.co/transformers/model_doc/bert.html#bertformaskedlm)."]},{"cell_type":"markdown","metadata":{"id":"NS7Sjv3ae3er"},"source":["# Preporcess True Dataset"]},{"cell_type":"code","metadata":{"id":"s66cqrKps5F2","executionInfo":{"status":"ok","timestamp":1618842995065,"user_tz":-420,"elapsed":2888,"user":{"displayName":"Wisaroot Lertthaweedech","photoUrl":"","userId":"18087249014958546013"}}},"source":["_ = !wget --no-check-certificate https://www.dropbox.com/s/37u83g55p19kvrl/clean-phone-data-for-students.csv"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"FdBvPtcmpEbg","executionInfo":{"status":"ok","timestamp":1618842995065,"user_tz":-420,"elapsed":2884,"user":{"displayName":"Wisaroot Lertthaweedech","photoUrl":"","userId":"18087249014958546013"}}},"source":["import pandas as pd\n","import numpy as np"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"Li9BUdp4fUxS","executionInfo":{"status":"ok","timestamp":1618842995066,"user_tz":-420,"elapsed":2882,"user":{"displayName":"Wisaroot Lertthaweedech","photoUrl":"","userId":"18087249014958546013"}}},"source":["true_data_df = pd.read_csv('clean-phone-data-for-students.csv')"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"if-YVf1VpIf0","executionInfo":{"status":"ok","timestamp":1618842995067,"user_tz":-420,"elapsed":2877,"user":{"displayName":"Wisaroot Lertthaweedech","photoUrl":"","userId":"18087249014958546013"}},"outputId":"c24b7b94-9b09-4270-e4fc-542b4599a5d6"},"source":["true_data_df.head()"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Sentence Utterance</th>\n","      <th>Action</th>\n","      <th>Object</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>&lt;PHONE_NUMBER_REMOVED&gt; ผมไปจ่ายเงินที่ Counte...</td>\n","      <td>enquire</td>\n","      <td>payment</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>internet ยังความเร็วอยุ่เท่าไหร ครับ</td>\n","      <td>enquire</td>\n","      <td>package</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>ตะกี้ไปชำระค่าบริการไปแล้ว แต่ยังใช้งานไม่ได้...</td>\n","      <td>report</td>\n","      <td>suspend</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>พี่ค่ะยังใช้ internet ไม่ได้เลยค่ะ เป็นเครื่อ...</td>\n","      <td>enquire</td>\n","      <td>internet</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>ฮาโหล คะ พอดีว่าเมื่อวานเปิดซิมทรูมูฟ แต่มันโ...</td>\n","      <td>report</td>\n","      <td>phone_issues</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                  Sentence Utterance   Action        Object\n","0   <PHONE_NUMBER_REMOVED> ผมไปจ่ายเงินที่ Counte...  enquire       payment\n","1               internet ยังความเร็วอยุ่เท่าไหร ครับ  enquire       package\n","2   ตะกี้ไปชำระค่าบริการไปแล้ว แต่ยังใช้งานไม่ได้...   report       suspend\n","3   พี่ค่ะยังใช้ internet ไม่ได้เลยค่ะ เป็นเครื่อ...  enquire      internet\n","4   ฮาโหล คะ พอดีว่าเมื่อวานเปิดซิมทรูมูฟ แต่มันโ...   report  phone_issues"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":292},"id":"g_VEt6jCpiHj","executionInfo":{"status":"ok","timestamp":1618842995068,"user_tz":-420,"elapsed":2872,"user":{"displayName":"Wisaroot Lertthaweedech","photoUrl":"","userId":"18087249014958546013"}},"outputId":"66028781-82d5-4bbc-9013-8210472b2473"},"source":["display(true_data_df['Object'].unique())\n","display(true_data_df.describe())"],"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":["array(['payment', 'package', 'suspend', 'internet', 'phone_issues',\n","       'service', 'nonTrueMove', 'balance', 'detail', 'bill', 'credit',\n","       'promotion', 'mobile_setting', 'iservice', 'roaming', 'truemoney',\n","       'information', 'lost_stolen', 'balance_minutes', 'idd',\n","       'TrueMoney', 'garbage', 'Payment', 'IDD', 'ringtone', 'Idd',\n","       'rate', 'loyalty_card', 'contact', 'officer', 'Balance', 'Service',\n","       'Loyalty_card'], dtype=object)"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Sentence Utterance</th>\n","      <th>Action</th>\n","      <th>Object</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>16175</td>\n","      <td>16175</td>\n","      <td>16175</td>\n","    </tr>\n","    <tr>\n","      <th>unique</th>\n","      <td>13389</td>\n","      <td>10</td>\n","      <td>33</td>\n","    </tr>\n","    <tr>\n","      <th>top</th>\n","      <td>บริการอื่นๆ</td>\n","      <td>enquire</td>\n","      <td>service</td>\n","    </tr>\n","    <tr>\n","      <th>freq</th>\n","      <td>97</td>\n","      <td>10377</td>\n","      <td>2525</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       Sentence Utterance   Action   Object\n","count               16175    16175    16175\n","unique              13389       10       33\n","top           บริการอื่นๆ  enquire  service\n","freq                   97    10377     2525"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":275},"id":"qiHVsIIupa1r","executionInfo":{"status":"ok","timestamp":1618842995069,"user_tz":-420,"elapsed":2868,"user":{"displayName":"Wisaroot Lertthaweedech","photoUrl":"","userId":"18087249014958546013"}},"outputId":"248bfd77-66bb-40c0-bdc5-d54af879885b"},"source":["true_data_df['Object']=true_data_df['Object'].str.lower().copy()\n","display(true_data_df['Object'].unique())\n","display(true_data_df.describe())"],"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":["array(['payment', 'package', 'suspend', 'internet', 'phone_issues',\n","       'service', 'nontruemove', 'balance', 'detail', 'bill', 'credit',\n","       'promotion', 'mobile_setting', 'iservice', 'roaming', 'truemoney',\n","       'information', 'lost_stolen', 'balance_minutes', 'idd', 'garbage',\n","       'ringtone', 'rate', 'loyalty_card', 'contact', 'officer'],\n","      dtype=object)"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Sentence Utterance</th>\n","      <th>Action</th>\n","      <th>Object</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>16175</td>\n","      <td>16175</td>\n","      <td>16175</td>\n","    </tr>\n","    <tr>\n","      <th>unique</th>\n","      <td>13389</td>\n","      <td>10</td>\n","      <td>26</td>\n","    </tr>\n","    <tr>\n","      <th>top</th>\n","      <td>บริการอื่นๆ</td>\n","      <td>enquire</td>\n","      <td>service</td>\n","    </tr>\n","    <tr>\n","      <th>freq</th>\n","      <td>97</td>\n","      <td>10377</td>\n","      <td>2528</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       Sentence Utterance   Action   Object\n","count               16175    16175    16175\n","unique              13389       10       26\n","top           บริการอื่นๆ  enquire  service\n","freq                   97    10377     2528"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"uo5PIxoP3uTk","executionInfo":{"status":"ok","timestamp":1618842995576,"user_tz":-420,"elapsed":3373,"user":{"displayName":"Wisaroot Lertthaweedech","photoUrl":"","userId":"18087249014958546013"}}},"source":["unique_object = true_data_df['Object'].unique()\n","\n","object_2_num_map = dict(zip(unique_object, range(len(unique_object))))\n","num_2_object_map = dict(zip(range(len(unique_object)), unique_object))"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":901},"id":"4RgVMP1k31Gq","executionInfo":{"status":"ok","timestamp":1618842995577,"user_tz":-420,"elapsed":3368,"user":{"displayName":"Wisaroot Lertthaweedech","photoUrl":"","userId":"18087249014958546013"}},"outputId":"2bbcc0e1-3259-432d-9c95-71e4ca86dd59"},"source":["display(object_2_num_map)\n","display(num_2_object_map)"],"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":["{'balance': 7,\n"," 'balance_minutes': 18,\n"," 'bill': 9,\n"," 'contact': 24,\n"," 'credit': 10,\n"," 'detail': 8,\n"," 'garbage': 20,\n"," 'idd': 19,\n"," 'information': 16,\n"," 'internet': 3,\n"," 'iservice': 13,\n"," 'lost_stolen': 17,\n"," 'loyalty_card': 23,\n"," 'mobile_setting': 12,\n"," 'nontruemove': 6,\n"," 'officer': 25,\n"," 'package': 1,\n"," 'payment': 0,\n"," 'phone_issues': 4,\n"," 'promotion': 11,\n"," 'rate': 22,\n"," 'ringtone': 21,\n"," 'roaming': 14,\n"," 'service': 5,\n"," 'suspend': 2,\n"," 'truemoney': 15}"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["{0: 'payment',\n"," 1: 'package',\n"," 2: 'suspend',\n"," 3: 'internet',\n"," 4: 'phone_issues',\n"," 5: 'service',\n"," 6: 'nontruemove',\n"," 7: 'balance',\n"," 8: 'detail',\n"," 9: 'bill',\n"," 10: 'credit',\n"," 11: 'promotion',\n"," 12: 'mobile_setting',\n"," 13: 'iservice',\n"," 14: 'roaming',\n"," 15: 'truemoney',\n"," 16: 'information',\n"," 17: 'lost_stolen',\n"," 18: 'balance_minutes',\n"," 19: 'idd',\n"," 20: 'garbage',\n"," 21: 'ringtone',\n"," 22: 'rate',\n"," 23: 'loyalty_card',\n"," 24: 'contact',\n"," 25: 'officer'}"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":459},"id":"N5tVhUtp3451","executionInfo":{"status":"ok","timestamp":1618842995578,"user_tz":-420,"elapsed":3364,"user":{"displayName":"Wisaroot Lertthaweedech","photoUrl":"","userId":"18087249014958546013"}},"outputId":"a6826aaf-795a-4b4f-c820-f33ab1e7d552"},"source":["print(\"Before Mappings\")\n","display(true_data_df['Object'])\n","true_data_df['Object'] = np.vectorize(object_2_num_map.get)(true_data_df['Object'])\n","\n","print(\"After Mappings\")\n","display(true_data_df['Object'])"],"execution_count":19,"outputs":[{"output_type":"stream","text":["Before Mappings\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/plain":["0             payment\n","1             package\n","2             suspend\n","3            internet\n","4        phone_issues\n","             ...     \n","16170        internet\n","16171             idd\n","16172         balance\n","16173         balance\n","16174         package\n","Name: Object, Length: 16175, dtype: object"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["After Mappings\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/plain":["0         0\n","1         1\n","2         2\n","3         3\n","4         4\n","         ..\n","16170     3\n","16171    19\n","16172     7\n","16173     7\n","16174     1\n","Name: Object, Length: 16175, dtype: int64"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"X-Aw5uZpudqn"},"source":["# Tokenize"]},{"cell_type":"code","metadata":{"id":"mv8EgtUTumPu","executionInfo":{"status":"ok","timestamp":1618843652097,"user_tz":-420,"elapsed":6550,"user":{"displayName":"Wisaroot Lertthaweedech","photoUrl":"","userId":"18087249014958546013"}}},"source":["# Install thai2transformer \n","# _ = !pip -q install torch==1.4.0 torchtext==0.4.0 torchvision==0.6.0\n","# _ = !pip -q install transformers==3.5.0 thai2transformers==0.1.2\n","\n","_ = !pip -q install torch==1.4.0 transformers==3.5.0 thai2transformers==0.1.2"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"rJKk06Soiscn","colab":{"base_uri":"https://localhost:8080/","height":511},"executionInfo":{"status":"error","timestamp":1618843655218,"user_tz":-420,"elapsed":635,"user":{"displayName":"Wisaroot Lertthaweedech","photoUrl":"","userId":"18087249014958546013"}},"outputId":"699da36e-b059-415c-9270-2ebb08e2c348"},"source":["import numpy as np\n","from tqdm.auto import tqdm\n","import torch\n","\n","#datasets\n","from datasets import load_dataset\n","\n","#transformers\n","import transformers\n","from transformers import (\n","    CamembertTokenizer,\n","    AutoTokenizer,\n","    AutoModel,\n","    AutoModelForMaskedLM,\n","    AutoConfig,\n","    AutoModelForSequenceClassification,\n","    AutoModelForTokenClassification,\n","    TrainingArguments,\n","    Trainer,\n","    pipeline,\n",")\n","\n","#thai2transformers\n","import thai2transformers\n","from thai2transformers.preprocess import process_transformers\n","from thai2transformers.metrics import (\n","    classification_metrics, \n","    multilabel_classification_metrics,\n",")\n","from thai2transformers.tokenizers import (\n","    ThaiRobertaTokenizer,\n","    ThaiWordsNewmmTokenizer,\n","    ThaiWordsSyllableTokenizer,\n","    FakeSefrCutTokenizer,\n","    SEFR_SPLIT_TOKEN\n",")\n","\n","from transformers import DistilBertForSequenceClassification, RobertaForSequenceClassification"],"execution_count":25,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-25-2d56953bdbfc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#transformers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m from transformers import (\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mCamembertTokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/camembert/__init__.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers.models.camembert.tokenization_camembert'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"cell_type":"code","metadata":{"id":"iiM1lDTdjY_p"},"source":["# model_names = [\n","#     'wangchanberta-base-att-spm-uncased',\n","#     'xlm-roberta-base',\n","#     'bert-base-multilingual-cased',\n","#     'wangchanberta-base-wiki-newmm',\n","#     'wangchanberta-base-wiki-ssg',\n","#     'wangchanberta-base-wiki-sefr',\n","#     'wangchanberta-base-wiki-spm',\n","# ]\n","\n","# tokenizers = {\n","#     'wangchanberta-base-att-spm-uncased': AutoTokenizer,\n","#     'xlm-roberta-base': AutoTokenizer,\n","#     'bert-base-multilingual-cased': AutoTokenizer,\n","#     'wangchanberta-base-wiki-newmm': ThaiWordsNewmmTokenizer,\n","#     'wangchanberta-base-wiki-ssg': ThaiWordsSyllableTokenizer,\n","#     'wangchanberta-base-wiki-sefr': FakeSefrCutTokenizer,\n","#     'wangchanberta-base-wiki-spm': ThaiRobertaTokenizer,\n","# }\n","# public_models = ['xlm-roberta-base', 'bert-base-multilingual-cased'] \n","# #@title Choose Pretrained Model\n","# model_name = \"xlm-roberta-base\" #@param [\"wangchanberta-base-att-spm-uncased\", \"xlm-roberta-base\", \"bert-base-multilingual-cased\", \"wangchanberta-base-wiki-newmm\", \"wangchanberta-base-wiki-syllable\", \"wangchanberta-base-wiki-sefr\", \"wangchanberta-base-wiki-spm\"]\n","\n","# #create tokenizer\n","# tokenizer = tokenizers[model_name].from_pretrained(\n","#                 f'airesearch/{model_name}' if model_name not in public_models else f'{model_name}',\n","#                 revision='main',\n","#                 model_max_length=416,)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EwA5KKYlkEMg","executionInfo":{"status":"ok","timestamp":1617546008229,"user_tz":-420,"elapsed":3984,"user":{"displayName":"Wisaroot Lertthaweedech","photoUrl":"","userId":"18087249014958546013"}},"outputId":"b523d977-a4c5-4dec-cfbd-be3a917cdebe"},"source":["tokenizer = AutoTokenizer.from_pretrained(\"airesearch/wangchanberta-base-att-spm-uncased\")\n","\n","true_X_encodings = tokenizer(list(true_data_df['Sentence Utterance']), add_special_tokens=True,\n","            max_length=512,\n","            pad_to_max_length=True,\n","            return_token_type_ids=True,truncation=True\n","        )"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"TvAg2RbYlcGd"},"source":["true_X = [np.array(true_X_encodings['input_ids']), np.array(true_X_encodings['attention_mask'])]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nrvh1A5wqEwP"},"source":["# Model initialization"]},{"cell_type":"code","metadata":{"id":"fR5k4CgosEXY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617546031146,"user_tz":-420,"elapsed":17670,"user":{"displayName":"Wisaroot Lertthaweedech","photoUrl":"","userId":"18087249014958546013"}},"outputId":"cc02f583-52ae-46b6-abe0-76c60c72fc82"},"source":["num_labels = len(object_2_num_map)\n","\n","# tokenizer = AutoTokenizer.from_pretrained(\"airesearch/wangchanberta-base-att-spm-uncased\")\n","model = AutoModelForSequenceClassification.from_pretrained(\"airesearch/wangchanberta-base-att-spm-uncased\", num_labels= num_labels)\n","model = torch.nn.DataParallel(model.cuda(), device_ids=[0])\n","\n","LEARNING_RATE =  1e-5\n","loss_fn = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased were not used when initializing CamembertForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"njjLI79ZsP5C"},"source":["def batch_data_generator(data, label, bs = 8, training = True):\n","  while(True):\n","    X1= []\n","    X2 = []\n","    Y = []\n","    from sklearn.utils import shuffle\n","    ids, masks = data[0], data[1]\n","    if(training):\n","      ids, masks, label = shuffle(ids, masks, label, random_state = 42)\n","    for a, b, c in zip(ids, masks, label):\n","      X1.append(a)\n","      X2.append(b)\n","      Y.append(c)\n","      if(len(X1) == bs):\n","        yield [np.array(X1), np.array(X2)], np.array(Y)\n","        X1= []\n","        X2 = []\n","        Y = []\n","    if(len(X1) > 0):\n","      yield [np.array(X1), np.array(X2)], np.array(Y)\n","    if(not training):\n","      yield None\n","      break"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MbFj48HIus0F"},"source":["true_X2 = np.swapaxes(np.array(true_X), 0, 1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RkmgD_dKtllj"},"source":["from sklearn.model_selection import train_test_split\n","\n","true_train_X, true_val_X, true_train_Y, true_val_Y = train_test_split(true_X2, true_data_df['Object'], test_size=.4)\n","true_test_X, true_val_X, true_test_Y, true_val_Y = train_test_split(true_val_X, true_val_Y, test_size=.5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hxo2b-mD1aur"},"source":["true_train_X = np.swapaxes(np.array(true_train_X), 0, 1)\n","true_test_X = np.swapaxes(np.array(true_test_X,), 0, 1)\n","true_val_X = np.swapaxes(np.array(true_val_X), 0, 1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bjo0-2RwuT_P"},"source":["train_generator = batch_data_generator(true_train_X, true_train_Y, training = True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"12O4wGJ9vjzB","executionInfo":{"status":"ok","timestamp":1617546088478,"user_tz":-420,"elapsed":558,"user":{"displayName":"Wisaroot Lertthaweedech","photoUrl":"","userId":"18087249014958546013"}},"outputId":"0651910b-e9c2-4eba-93f5-044741304b64"},"source":["dummy_generator = batch_data_generator(true_train_X, true_train_Y, training = True)\n","X_dummy, Y_dummy = next(dummy_generator)\n","print(X_dummy[0].shape, X_dummy[1].shape, Y_dummy.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(8, 512) (8, 512) (8,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":489,"referenced_widgets":["8f4acbfa1da74be3a5b0cfc4949867d8","9b94b6b3dd8a4adaa12a78a38e2a997a","7fbee391ccd34b1d8cb7b51201c561d6","b1882c5d039a414cb44664caca2ad800","45dceb81dcfb4966a1fd1932dbae2036","d05e2a5069384c86b259d2d46e7483e8","3c694ff844a345ea9e419308ef8e6f36","4053f5f52ff441c6ace44ba3dbd779b1"]},"id":"Y6HLIQ8Uyud3","executionInfo":{"status":"ok","timestamp":1617546647801,"user_tz":-420,"elapsed":559871,"user":{"displayName":"Wisaroot Lertthaweedech","photoUrl":"","userId":"18087249014958546013"}},"outputId":"8c80ba75-6911-4cc1-cf1d-9d8040e0f6bf"},"source":["device = \"cuda:0\"\n","from tqdm import tqdm_notebook\n","from sklearn.metrics import accuracy_score\n","from collections import deque \n","\n","train_acc_stat =  deque(maxlen = 100)\n","train_loss_stat =  deque(maxlen = 100)\n","\n","for step in  tqdm_notebook(range(1000)):\n","    X, Y = next(train_generator)\n","    ids = torch.tensor(X[0], dtype = torch.long, device = device)\n","    mask = torch.tensor(X[1], dtype = torch.long, device = device)\n","    targets = torch.tensor(Y, dtype = torch.long).to(device)\n","\n","    optimizer.zero_grad()\n","    outputs = model(ids, mask)\n","    # loss = loss_fn(outputs['logits'], targets)\n","    loss = loss_fn(outputs[0], targets)\n","    \n","    loss.backward()\n","    optimizer.step()\n","\n","    with torch.no_grad():\n","      # train_acc = accuracy_score(Y, outputs['logits'].argmax(axis = 1).cpu().detach().numpy() )\n","      train_acc = accuracy_score(Y, outputs[0].argmax(axis = 1).cpu().detach().numpy() )\n","      train_loss = loss.cpu().detach().numpy()\n","      train_acc_stat.append(train_acc)\n","      train_loss_stat.append(train_loss)\n","\n","    if (step + 1) %100==0:\n","      print(\"iter = {} train_acc = {}\".format(step, np.array(train_acc_stat).mean()))\n","      print(\"iter = {} train_loss = {}\".format(step, np.array(train_loss_stat).mean()))\n","\n","\n","    if (step + 1) %500==0:\n","      #validation step\n","      with torch.no_grad():\n","        val_generator = batch_data_generator(true_val_X, true_val_Y, training = False)\n","        y_true = []\n","        y_pred = []\n","        while(True):\n","          d = next(val_generator)\n","          if(d is None): break\n","          X, Y = d\n","          ids = torch.tensor(X[0], dtype = torch.long, device = device)\n","          mask = torch.tensor(X[1], dtype = torch.long, device = device)\n","          # outputs_cls = model(ids, mask)['logits'].argmax(axis = 1).cpu().detach().numpy()\n","          outputs_cls = model(ids, mask)[0].argmax(axis = 1).cpu().detach().numpy()\n","          y_true.append(Y)\n","          y_pred.append(outputs_cls)\n","        y_true = np.concatenate(y_true)\n","        y_pred = np.concatenate(y_pred)\n","        print(\"val acc\", accuracy_score(y_true, y_pred))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n","Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n","  if __name__ == '__main__':\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8f4acbfa1da74be3a5b0cfc4949867d8","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["iter = 99 train_acc = 0.15125\n","iter = 99 train_loss = 2.9082915782928467\n","iter = 199 train_acc = 0.2775\n","iter = 199 train_loss = 2.5786659717559814\n","iter = 299 train_acc = 0.32875\n","iter = 299 train_loss = 2.3735148906707764\n","iter = 399 train_acc = 0.36375\n","iter = 399 train_loss = 2.219857692718506\n","iter = 499 train_acc = 0.355\n","iter = 499 train_loss = 2.243892192840576\n","iter = 599 train_acc = 0.4\n","iter = 599 train_loss = 2.10455584526062\n","iter = 699 train_acc = 0.4625\n","iter = 699 train_loss = 1.8929330110549927\n","iter = 799 train_acc = 0.5225\n","iter = 799 train_loss = 1.6740680932998657\n","iter = 899 train_acc = 0.53625\n","iter = 899 train_loss = 1.6326984167099\n","iter = 999 train_acc = 0.54375\n","iter = 999 train_loss = 1.6138421297073364\n","val acc 0.4970633693972179\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZMyW4MtpQs2a","executionInfo":{"status":"ok","timestamp":1617546716757,"user_tz":-420,"elapsed":58536,"user":{"displayName":"Wisaroot Lertthaweedech","photoUrl":"","userId":"18087249014958546013"}},"outputId":"4dc923db-f946-4ba6-e77f-184c680b9db9"},"source":["test_generator = batch_data_generator(true_test_X, true_test_Y, training = False)\n","y_true = []\n","y_pred = []\n","while(True):\n","  d = next(test_generator)\n","  if(d is None): break\n","  X, Y = d\n","  ids = torch.tensor(X[0], dtype = torch.long, device = device)\n","  mask = torch.tensor(X[1], dtype = torch.long, device = device)\n","  outputs_cls = model(ids, mask)[0].argmax(axis = 1).cpu().detach().numpy()\n","  y_true.append(Y)\n","  y_pred.append(outputs_cls)\n","y_true = np.concatenate(y_true)\n","y_pred = np.concatenate(y_pred)\n","print(\"test acc\", accuracy_score(y_true, y_pred))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["test acc 0.4843894899536321\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Tum9kKIfTC-W"},"source":["**<font color='red'>Comparing to my  non-transformer model in HW 6 which I got 30.8% accuracy,  the model fine-tuned using pretrained WangchanBERTa gives a significant higher accuracy at 48.4%.</font>**"]}]}